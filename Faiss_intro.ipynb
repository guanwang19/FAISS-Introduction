{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Windows, run anaconda prompt as admin\n",
    "# conda install -c conda-forge faiss-cpu (as of 20221008)\n",
    "# conda install -c services sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pair_ID\\tsentence_A\\tsentence_B\\trelatedness_score\\tentailment_judgment\\n1\\tA group of kids is playing in a yard and an old man is standing in the background\\tA group of boys in a yard is playing and a man is standing in the background\\t4.5\\tNEUTRAL\\n2\\tA group of children is playing in the house and there is no man standing in the background\\tA group of kids is playing in a yard and an old man is standing in the background\\t3.2\\tNEUTRAL\\n3\\tThe young boys are playing outdoors and the man is smiling nearby\\tThe '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get('https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/sick2014/SICK_train.txt')\n",
    "\n",
    "text = res.text\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need this in a dataframe, which we build from the `text` string like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_ID</th>\n",
       "      <th>sentence_A</th>\n",
       "      <th>sentence_B</th>\n",
       "      <th>relatedness_score</th>\n",
       "      <th>entailment_judgment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>ENTAILMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>The kids are playing outdoors near a man with ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.4</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_ID                                         sentence_A  \\\n",
       "0        1  A group of kids is playing in a yard and an ol...   \n",
       "1        2  A group of children is playing in the house an...   \n",
       "2        3  The young boys are playing outdoors and the ma...   \n",
       "3        5  The kids are playing outdoors near a man with ...   \n",
       "4        9  The young boys are playing outdoors and the ma...   \n",
       "\n",
       "                                          sentence_B  relatedness_score  \\\n",
       "0  A group of boys in a yard is playing and a man...                4.5   \n",
       "1  A group of kids is playing in a yard and an ol...                3.2   \n",
       "2  The kids are playing outdoors near a man with ...                4.7   \n",
       "3  A group of kids is playing in a yard and an ol...                3.4   \n",
       "4  A group of kids is playing in a yard and an ol...                3.7   \n",
       "\n",
       "  entailment_judgment  \n",
       "0             NEUTRAL  \n",
       "1             NEUTRAL  \n",
       "2          ENTAILMENT  \n",
       "3             NEUTRAL  \n",
       "4             NEUTRAL  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(StringIO(text), sep='\\t')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take all samples from `sentence_A` and build sentence embeddings for each - which we can then store in FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A group of kids is playing in a yard and an old man is standing in the background',\n",
       " 'A group of children is playing in the house and there is no man standing in the background',\n",
       " 'The young boys are playing outdoors and the man is smiling nearby',\n",
       " 'The kids are playing outdoors near a man with a smile',\n",
       " 'The young boys are playing outdoors and the man is smiling nearby']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = data['sentence_A'].tolist()\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will pull in the `sentence_B` column too, giving us ~4.5K *unique* sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4802"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_b = data['sentence_B'].tolist()\n",
    "sentences.extend(sentence_b)\n",
    "len(set(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't a particularly large number, so let's pull in a few more similar datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/MSRpar.train.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/MSRpar.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2013/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2014/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2014/images.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/images.test.tsv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gremw\\AppData\\Local\\Temp\\ipykernel_33796\\3737715271.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(StringIO(res.text), sep='\\t', header=None, error_bad_lines=False)\n",
      "b'Skipping line 191: expected 3 fields, saw 4\\nSkipping line 206: expected 3 fields, saw 4\\nSkipping line 295: expected 3 fields, saw 4\\nSkipping line 695: expected 3 fields, saw 4\\nSkipping line 699: expected 3 fields, saw 4\\n'\n",
      "C:\\Users\\gremw\\AppData\\Local\\Temp\\ipykernel_33796\\3737715271.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(StringIO(res.text), sep='\\t', header=None, error_bad_lines=False)\n",
      "b'Skipping line 104: expected 3 fields, saw 4\\nSkipping line 181: expected 3 fields, saw 4\\nSkipping line 317: expected 3 fields, saw 4\\nSkipping line 412: expected 3 fields, saw 5\\nSkipping line 508: expected 3 fields, saw 4\\n'\n",
      "C:\\Users\\gremw\\AppData\\Local\\Temp\\ipykernel_33796\\3737715271.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(StringIO(res.text), sep='\\t', header=None, error_bad_lines=False)\n",
      "C:\\Users\\gremw\\AppData\\Local\\Temp\\ipykernel_33796\\3737715271.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(StringIO(res.text), sep='\\t', header=None, error_bad_lines=False)\n",
      "C:\\Users\\gremw\\AppData\\Local\\Temp\\ipykernel_33796\\3737715271.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(StringIO(res.text), sep='\\t', header=None, error_bad_lines=False)\n",
      "C:\\Users\\gremw\\AppData\\Local\\Temp\\ipykernel_33796\\3737715271.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(StringIO(res.text), sep='\\t', header=None, error_bad_lines=False)\n",
      "C:\\Users\\gremw\\AppData\\Local\\Temp\\ipykernel_33796\\3737715271.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(StringIO(res.text), sep='\\t', header=None, error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "for url in urls:\n",
    "    res = requests.get(url)\n",
    "    # extract to dataframe\n",
    "    data = pd.read_csv(StringIO(res.text), sep='\\t', header=None, error_bad_lines=False)\n",
    "    # add to columns 1 and 2 to sentences list\n",
    "    sentences.extend(data[1].tolist())\n",
    "    sentences.extend(data[2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12849"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before converting to our sentence embeddings, we will save to text file as backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates and NaN\n",
    "sentences = [\n",
    "    sentence.replace('\\n', '') for sentence in list(set(sentences)) if type(sentence) is str\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12848"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentences.txt', 'w') as fp:\n",
    "    fp.write('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 14.5K *unique* sentences, a much better size. We'll go ahead and build the sentence embeddings (this can take some time, feel free to download the embeddings from [here]()). TODO add link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3affe67c2b54600ae7bc1635abb5116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/391 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23acfb7eb934d6fa8852beddd96da1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82404c9abe644beb8a5b5bbacbf5e3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "383fc2502d924866a27f9f619bdb2ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd605804d760411081ffee666c3b5513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c182a05d4399462481127b70a224650c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee81f2ac5ae49c48c0b37e28d53fa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249d3ff07df8408a9c34fd4fad632bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c29d6f42d3f415cbc3e21177f4dbcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09bf25766154af4a85783fc24fc1ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9899c50fa5534a8394b5a3982b6b5920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0bd907dc744f7fb3b9345e9f61bb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3bf8c0a92843c8a80db6eba741e29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(12848, 768)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save/load from file in the case of needing to reload the notebook for any reason later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12848"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.6647695e-02,  4.6723583e-01,  3.6893499e-01, ...,\n",
       "         1.1601105e-01, -2.5436433e-02,  4.5156029e-01],\n",
       "       [ 9.1679931e-01,  5.1914793e-01, -6.4101094e-01, ...,\n",
       "        -2.9355940e-01,  1.4221726e-01,  4.4613871e-01],\n",
       "       [-5.8313829e-01,  2.6698965e-01,  2.2379859e-01, ...,\n",
       "         9.0821609e-02, -5.0137863e-02, -1.4212216e-01],\n",
       "       [ 3.7224758e-01, -5.7472223e-01,  2.3755560e+00, ...,\n",
       "        -4.3555823e-01, -5.3475142e-01, -1.0316626e-03],\n",
       "       [ 1.8525401e-01, -1.3984743e-01,  7.8827852e-01, ...,\n",
       "         1.4278315e+00, -3.6858153e-02, -8.6588544e-01]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./sim_sentences/embeddings_X.npy', 'wb') as fp:\n",
    "    np.save(fp, sentence_embeddings[0:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_0.npy | 0 -> 256\n",
      "embeddings_1.npy | 256 -> 512\n",
      "embeddings_2.npy | 512 -> 768\n",
      "embeddings_3.npy | 768 -> 1024\n",
      "embeddings_4.npy | 1024 -> 1280\n",
      "embeddings_5.npy | 1280 -> 1536\n",
      "embeddings_6.npy | 1536 -> 1792\n",
      "embeddings_7.npy | 1792 -> 2048\n",
      "embeddings_8.npy | 2048 -> 2304\n",
      "embeddings_9.npy | 2304 -> 2560\n",
      "embeddings_10.npy | 2560 -> 2816\n",
      "embeddings_11.npy | 2816 -> 3072\n",
      "embeddings_12.npy | 3072 -> 3328\n",
      "embeddings_13.npy | 3328 -> 3584\n",
      "embeddings_14.npy | 3584 -> 3840\n",
      "embeddings_15.npy | 3840 -> 4096\n",
      "embeddings_16.npy | 4096 -> 4352\n",
      "embeddings_17.npy | 4352 -> 4608\n",
      "embeddings_18.npy | 4608 -> 4864\n",
      "embeddings_19.npy | 4864 -> 5120\n",
      "embeddings_20.npy | 5120 -> 5376\n",
      "embeddings_21.npy | 5376 -> 5632\n",
      "embeddings_22.npy | 5632 -> 5888\n",
      "embeddings_23.npy | 5888 -> 6144\n",
      "embeddings_24.npy | 6144 -> 6400\n",
      "embeddings_25.npy | 6400 -> 6656\n",
      "embeddings_26.npy | 6656 -> 6912\n",
      "embeddings_27.npy | 6912 -> 7168\n",
      "embeddings_28.npy | 7168 -> 7424\n",
      "embeddings_29.npy | 7424 -> 7680\n",
      "embeddings_30.npy | 7680 -> 7936\n",
      "embeddings_31.npy | 7936 -> 8192\n",
      "embeddings_32.npy | 8192 -> 8448\n",
      "embeddings_33.npy | 8448 -> 8704\n",
      "embeddings_34.npy | 8704 -> 8960\n",
      "embeddings_35.npy | 8960 -> 9216\n",
      "embeddings_36.npy | 9216 -> 9472\n",
      "embeddings_37.npy | 9472 -> 9728\n",
      "embeddings_38.npy | 9728 -> 9984\n",
      "embeddings_39.npy | 9984 -> 10240\n",
      "embeddings_40.npy | 10240 -> 10496\n",
      "embeddings_41.npy | 10496 -> 10752\n",
      "embeddings_42.npy | 10752 -> 11008\n",
      "embeddings_43.npy | 11008 -> 11264\n",
      "embeddings_44.npy | 11264 -> 11520\n",
      "embeddings_45.npy | 11520 -> 11776\n",
      "embeddings_46.npy | 11776 -> 12032\n",
      "embeddings_47.npy | 12032 -> 12288\n",
      "embeddings_48.npy | 12288 -> 12544\n",
      "embeddings_49.npy | 12544 -> 12800\n",
      "embeddings_50.npy | 12800 -> 12849\n"
     ]
    }
   ],
   "source": [
    "# saving data\n",
    "split = 256\n",
    "file_count = 0\n",
    "for i in range(0, sentence_embeddings.shape[0], split):\n",
    "    end = i + split\n",
    "    if end > sentence_embeddings.shape[0] + 1:\n",
    "        end = sentence_embeddings.shape[0] + 1\n",
    "    file_count = '0' + str(file_count) if file_count < 0 else str(file_count)\n",
    "    with open(f'./sim_sentences/embeddings_{file_count}.npy', 'wb') as fp:\n",
    "        np.save(fp, sentence_embeddings[i:end, :])\n",
    "    print(f\"embeddings_{file_count}.npy | {i} -> {end}\")\n",
    "    file_count = int(file_count) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12848"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We setup our FAISS database dimensionality (number of dimensions per vector) based on these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = sentence_embeddings.shape[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flat L2 Index\n",
    "\n",
    "We initialize the flat L2 distance index `IndexFlatL2`, all we need is the specify the vector dimensionality - which in this case is `d == 768` (to align with the sentence-BERT model output embeddings of size `768`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "faiss.swigfaiss.IndexFlatL2"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x000001DA8005DDB0> >"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, we will use indexes that require us to `train` them on our data before being used (if we are grouping or transforming the data in any way). `IndexFlatL2` however, is a simple operation and only requires that we calculate distances between vectors when we introduce our query vector `xq` during search. So, in this case, no training is required - which we can confirm by checking the `is_trained` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so once we're happy that our index is prepared, we then add new vectors using the `add` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12848"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then search given a query `xq` and number of nearest neigbors to return `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "xq = model.encode([\"Someone sprints with a football\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5155 1477 7280 2089]]\n",
      "CPU times: total: 93.8 ms\n",
      "Wall time: 14.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)  # k-nearest neigbors of the query vector | nprobe == 1: 6495 26392 61709 49932 | nprobe == 10: 36245  6495 57489  8705"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're returning indices `7460`, `10940`, `3781`, and `5747`, which returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54.62374 , 54.853504, 57.356274, 58.870564]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5155: A group of football players is running in the field',\n",
       " '1477: A group of people playing football is running in the field',\n",
       " '7280: Two groups of people are playing football',\n",
       " '2089: A football player kicks the ball.']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'{i}: {sentences[i]}' for i in I[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A group of football players is running in the field'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[5155]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we have some good matches, everything returned includes people running with a football, or on the context of a football match. Now, if we'd rather extract the numerical vectors from FAISS, we can do that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = np.zeros((k, d))\n",
    "for i, val in enumerate(I[0].tolist()):\n",
    "    vecs[i, :] = index.reconstruct(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 768)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01627024,  0.22325903, -0.15037397, -0.30747244, -0.27122411,\n",
       "       -0.10593186, -0.06460925,  0.04738237, -0.73349077, -0.37657702,\n",
       "       -0.76762813,  0.16902898,  0.53107667,  0.51176614,  1.14415836,\n",
       "       -0.08562844, -0.67240077, -0.966371  ,  0.02545465, -0.21559812,\n",
       "       -1.25656605, -0.829822  , -0.09824988, -0.2185089 ,  0.50610238,\n",
       "        0.10527924,  0.50396854,  0.65242964, -1.39458692,  0.65847474,\n",
       "       -0.2152534 , -0.22487433,  0.8181836 ,  0.08464295, -0.76141697,\n",
       "       -0.28928307, -0.09825767, -0.73046178,  0.07855812, -0.84354568,\n",
       "       -0.59242064,  0.77471352, -1.20920539, -0.22757988, -1.30733621,\n",
       "       -0.23081498, -1.31322515,  0.01629087, -0.97285461,  0.19308169,\n",
       "        0.47424558,  1.18920898, -1.96741283, -0.70061105, -0.2963874 ,\n",
       "        0.60533714,  0.62407476, -0.70340377, -0.86754155,  0.17673109,\n",
       "       -0.19170527, -0.0295194 ,  0.22623539, -0.16695416, -0.80402541,\n",
       "       -0.4591893 ,  0.69675493, -0.24928212, -1.0147866 , -0.92174548,\n",
       "       -0.33842668, -0.39296791, -0.83734864, -0.11479249,  0.46049702,\n",
       "       -1.45211208,  0.60310435,  0.38696289, -0.04061216,  0.00453154,\n",
       "        0.24117839,  0.05396274,  0.07506427,  1.05115855,  0.12383968,\n",
       "       -0.71281123,  0.11722904,  0.52238202, -0.04581184,  0.26827082,\n",
       "        0.85985422, -0.35669884, -0.64667082, -0.54357916, -0.04310518,\n",
       "        0.95139217, -0.15605715, -0.49625322, -0.11140201,  0.15610144])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Adding Partitioning to the Index\n",
    "\n",
    "FAISS allows us to add an additional step to optimize our search efficiency using a variety of different methods. A popular approach is to partition the index into *[Voronoi cells](https://en.wikipedia.org/wiki/Voronoi_diagram)*. Using this method we would take our query vector `xq`, identify the *cell* it belongs to, and then use our `IndexFlatL2` to search between the query vector `xq` and all indexed vectors belonging to that *cell*. We can also include vectors from other nearby cells too.\n",
    "\n",
    "We initialize our new partitioned index by first adding our previous `IndexFlatL2` operation as a quantization step (another step in the search process), and feeding this into the new `IndexIVFFlat` operation like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlist = 50\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've added a new parameter `nlist`. We use `nlist` to define how many partitions we'd like our index to have. \n",
    "\n",
    "When we built the previous, `IndexFlatL2`-only index, we noted that no training was required as no grouping/transformation was required to build that index. Now that we've added partitioning using `IndexIVFFlat`, this is no longer the case. Let's take a look at the `is_trained` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we need to do now is `train` our index on our data, which we do *before* adding any data to the index (otherwise the index cannot know how to group each vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.train(sentence_embeddings)\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our index is trained, we add our data just as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12848"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.add(sentence_embeddings)\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's search again using the same indexed sentence embeddings and the same query `xq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5155 1477 7280 2089]]\n",
      "CPU times: total: 46.9 ms\n",
      "Wall time: 2.68 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can increase the number of nearby cells to search too with `nprobe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.nprobe = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5155 1477 7280 2089]]\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 4.47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of `nprobe` will improve the accuracy of our search, but cost time. Our earlier `IndexFlatL2`-only search was *exhaustive* (it compared every single vector) and so it identified the closest matches with a perfect accuracy. The smaller our `nprobe` value, the smaller scope that we search. We received perfect results (that matched our previous `IndexFlatL2`-only results - `7460`, `10940`, `3781`, `5747`), however, if we found that we were not getting closely matching results, we could simply bump `nprobe` up further - improving accuracy, but increasing time-taken too.\n",
    "\n",
    "It's worth noting that the time taken can change with each run too, if we rerun the above block, we usually get a different time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5155 1477 7280 2089]]\n",
      "CPU times: total: 62.5 ms\n",
      "Wall time: 3.13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For IVF (and IMI) indexes, before attempting to use the `reconstruct` method, we need to call the `make_direct_map` method - otherwise we will return a `RunetimeError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in __int64 __cdecl faiss::DirectMap::get(__int64) const at D:\\bld\\faiss-split_1613625922506\\work\\faiss\\invlists\\DirectMap.cpp:85: direct map not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [106]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7460\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\faiss\\__init__.py:190\u001b[0m, in \u001b[0;36mhandle_Index.<locals>.replacement_reconstruct\u001b[1;34m(self, key, x)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md, )\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreconstruct_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswig_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\faiss\\swigfaiss.py:2786\u001b[0m, in \u001b[0;36mIndexIVF.reconstruct\u001b[1;34m(self, key, recons)\u001b[0m\n\u001b[0;32m   2785\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreconstruct\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, recons):\n\u001b[1;32m-> 2786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_swigfaiss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexIVF_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecons\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error in __int64 __cdecl faiss::DirectMap::get(__int64) const at D:\\bld\\faiss-split_1613625922506\\work\\faiss\\invlists\\DirectMap.cpp:85: direct map not initialized"
     ]
    }
   ],
   "source": [
    "index.reconstruct(7460)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `make_direct_map`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.make_direct_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.53096849e-01, -2.53371239e-01,  4.62723851e-01,  2.72474557e-01,\n",
       "       -1.82537049e-01, -2.44593978e-01,  2.46149510e-01, -5.41305542e-01,\n",
       "        3.19927722e-01, -3.23612750e-01,  1.27549395e-01,  8.02246392e-01,\n",
       "       -1.48230180e-01,  9.33590606e-02, -1.03844798e+00,  5.34105062e-01,\n",
       "       -9.38271657e-02, -3.89643490e-01, -5.15782058e-01,  1.66627899e-01,\n",
       "        1.29660666e-01, -1.22047566e-01,  3.81940678e-02,  3.05460662e-01,\n",
       "        8.77040327e-01,  2.05136776e-01,  3.00248154e-02, -3.26555997e-01,\n",
       "       -8.86763811e-01,  3.64780307e-01,  1.68430969e-01,  8.41970682e-01,\n",
       "       -1.27686083e+00,  2.03377813e-01,  3.77641737e-01,  9.30500269e-01,\n",
       "        1.06446075e+00, -2.00449586e-01, -2.17186883e-02,  1.17594987e-01,\n",
       "       -1.79340579e-02,  5.59337325e-02, -2.81759411e-01,  3.30396473e-01,\n",
       "       -1.65603533e-01, -7.68142223e-01,  1.29712510e+00,  5.71367264e-01,\n",
       "        3.91046166e-01, -1.33302402e+00,  3.66828322e-01,  9.78994846e-01,\n",
       "        2.63698995e-01,  4.61863756e-01, -6.96462020e-02, -7.03230873e-02,\n",
       "        9.84127447e-02, -1.34393454e+00, -4.56808984e-01, -9.36726868e-01,\n",
       "       -1.66341722e+00,  1.27035484e-01, -2.93851160e-02, -2.38373473e-01,\n",
       "       -3.63545805e-01,  5.82072476e-04, -5.41261435e-01,  3.82704496e-01,\n",
       "       -4.44013774e-01, -1.17004669e+00, -4.56747383e-01, -6.21711254e-01,\n",
       "       -6.31515384e-01, -5.38346410e-01,  7.43872106e-01,  4.19525415e-01,\n",
       "       -5.00479996e-01,  4.15943921e-01,  4.69135046e-01, -6.08584955e-02,\n",
       "       -5.69103658e-01,  4.17686820e-01,  3.18314701e-01, -1.01411426e-02,\n",
       "        2.24590242e-01, -4.47599083e-01,  9.62411225e-01,  4.05087210e-02,\n",
       "        2.17371628e-01,  2.61417050e-02,  6.26132786e-02,  7.17573464e-02,\n",
       "        6.78722501e-01, -6.11224592e-01, -1.13811004e+00, -3.55506837e-01,\n",
       "        3.98301691e-01,  1.93055540e-01, -3.43698472e-01,  2.90073037e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.reconstruct(7460)[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now significantly reduced the search time, what can we do next?\n",
    "\n",
    "# Quantization\n",
    "\n",
    "Well, when storing these vectors we're storing the full (eg `Flat`) vector. Now in very big datasets this can quickly become a problem. Typically, we look at big datasets, and when working with large dataset we will find that storing the full vectors consumes too much space.\n",
    "\n",
    "Fortunately, FAISS comes with the ability to *compress* our vectors using transformations based on *Product Quantization* (PQ). But, what is PQ? Well, we can view it as an additional approximation step similar to our use of **IVF**, which allowed us to *approximate* by reducing the scope of our search. PQ is slightly different however, and approximates the distance (or similarity) calculation instead.\n",
    "\n",
    "[PQ explanation TODO REMOVE](https://mccormickml.com/2017/10/13/product-quantizer-tutorial-part-1/)\n",
    "\n",
    "PQ achieves this approximated distance operation by compressing the vectors themselves. This consists of a few steps.\n",
    "\n",
    "1. We split the every original vector into several subvectors.\n",
    "\n",
    "2. For each set of subvectors, we perform a clustering operation, creating many centroids for each subvector set.\n",
    "\n",
    "3. In our vector of subvectors, we replace each subvector with the ID of it's nearest centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 8  # number of centroid IDs in final compressed vectors\n",
    "bits = 8 # number of bits in each centroid\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(d)  # we keep the same L2 distance flat index\n",
    "index = faiss.IndexIVFPQ(quantizer, d, nlist, m, bits) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll need to `train` the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.train(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `add` our vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare it to our previous index *without* PQ, and an `nprobe` value of `10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.nprobe = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 245  469 2089 1477]]\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 1.84 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through adding PQ we've reduced our search time from ~7.5ms to ~5ms, a small difference on a dataset of this size, but when scaled to larger datasets this can make a huge difference.\n",
    "\n",
    "Now, we should also notice the slightly different results being returned. Beforehand with our exhaustive L2 search we were returning `7460`, `10940`, `3781`, and `5747`. Now, we see a slightly different order to our results - and two different vectors, `5013` and `5370`.\n",
    "\n",
    "Each of our speed optimization operations, `IVF` and `PQ`, come at the cost of accuracy. Now, if we print out these results we will nonetheless find that each item is still a relevant match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['245: A man in a football uniform is running with a football during a game.',\n",
       " '469: A group of football players running down the field.',\n",
       " '2089: A football player kicks the ball.',\n",
       " '1477: A group of people playing football is running in the field']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f'{i}: {sentences[i]}' for i in I[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So although we might not get the *perfect* result, we still get close - and thanks to the approximation, we get a significant speed boost"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a683edd788238e5c64f9fa2e4bdd4387776bc5c6f4f0a84da0685f9a25e421d6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
